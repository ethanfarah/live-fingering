{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "kSOcU-Sjo0Cw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from tqdm import tqdm\n",
        "import sklearn\n",
        "from torch.optim.lr_scheduler import OneCycleLR, MultiStepLR\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import math\n",
        "import random\n",
        "import sys\n",
        "import cv2\n",
        "import gc\n",
        "import glob\n",
        "import datetime\n",
        "import json\n",
        "import pyarrow.parquet as pq\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import Subset\n",
        "from torch.optim import Adam, AdamW, SGD, RMSprop, Adamax, Adadelta, Adagrad\n",
        "from torch.cuda import amp\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "WvpHaShAo1S7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CPU\n",
            "REPLICAS: 1\n",
            "Using device: cpu\n",
            "Number of replicas: 1\n",
            "Is TPU: False\n"
          ]
        }
      ],
      "source": [
        "def get_device_strategy(device='GPU'):\n",
        "    IS_TPU = False\n",
        "\n",
        "    if device == 'TPU':\n",
        "        # Note: TPU support in PyTorch requires torch_xla library, typically used on Google Cloud Platform.\n",
        "        try:\n",
        "            import torch_xla.core.xla_model as xm\n",
        "            device = xm.xla_device()\n",
        "            IS_TPU = True\n",
        "            print(\"Using TPU\")\n",
        "        except ImportError:\n",
        "            raise ImportError(\"TPU support requires the torch_xla library.\")\n",
        "    \n",
        "    elif device == 'GPU' or device == 'CPU' or device == 'MPS':\n",
        "        if torch.cuda.is_available() and device == 'GPU':\n",
        "            ngpu = torch.cuda.device_count()\n",
        "            if ngpu > 1:\n",
        "                print(\"Using multi GPU\")\n",
        "                device = torch.device('cuda')\n",
        "            elif ngpu == 1:\n",
        "                print(\"Using single GPU\")\n",
        "                device = torch.device('cuda')\n",
        "            else:\n",
        "                print(\"Using CPU\")\n",
        "                device = torch.device('cpu')\n",
        "        elif device == 'MPS' and torch.backends.mps.is_available():\n",
        "            print(\"Using MPS\")\n",
        "            device = torch.device('mps')\n",
        "        else:\n",
        "            print(\"Using CPU\")\n",
        "            device = torch.device('cpu')\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        ngpu = torch.cuda.device_count()\n",
        "        print(\"Num GPUs Available: \", ngpu)\n",
        "    elif device.type == 'mps':\n",
        "        ngpu = 1\n",
        "        print(\"Num MPS Devices Available: 1\")\n",
        "    else:\n",
        "        ngpu = 0\n",
        "\n",
        "    REPLICAS = ngpu if ngpu > 0 else 1\n",
        "    print(f'REPLICAS: {REPLICAS}')\n",
        "\n",
        "    return device, REPLICAS, IS_TPU\n",
        "\n",
        "device, N_REPLICAS, IS_TPU = get_device_strategy(device='GPU')\n",
        "\n",
        "# Output the device details\n",
        "print(f'Using device: {device}')\n",
        "print(f'Number of replicas: {N_REPLICAS}')\n",
        "print(f'Is TPU: {IS_TPU}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_dir = os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(os.path.join(base_dir, 'asl-signs', 'train.csv'))\n",
        "paths = train_df['path'].values\n",
        "paths = [os.path.join(base_dir, 'asl-signs', path) for path in paths]\n",
        "signs = train_df['sign'].values\n",
        "with open(os.path.join(base_dir, 'asl-signs', 'sign_to_prediction_index_map.json'), 'r') as f:\n",
        "    sign_to_prediction_index_map = json.load(f)\n",
        "\n",
        "def labels_to_ids(labels, mapping):\n",
        "    return [mapping[label] for label in labels]\n",
        "\n",
        "ids = labels_to_ids(signs, sign_to_prediction_index_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constants\n",
        "ROWS_PER_FRAME = 543\n",
        "MAX_LEN = 384\n",
        "NUM_CLASSES = 250\n",
        "PAD = -100.0\n",
        "NOSE = [1, 2, 98, 327]\n",
        "LNOSE = [98]\n",
        "RNOSE = [327]\n",
        "LIP = [\n",
        "    0, 61, 185, 40, 39, 37, 267, 269, 270, 409,\n",
        "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
        "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
        "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
        "]\n",
        "LLIP = [84, 181, 91, 146, 61, 185, 40, 39, 37, 87, 178, 88, 95, 78, 191, 80, 81, 82]\n",
        "RLIP = [314, 405, 321, 375, 291, 409, 270, 269, 267, 317, 402, 318, 324, 308, 415, 310, 311, 312]\n",
        "POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n",
        "LPOSE = [513, 505, 503, 501]\n",
        "RPOSE = [512, 504, 502, 500]\n",
        "REYE = [\n",
        "    33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
        "    246, 161, 160, 159, 158, 157, 173,\n",
        "]\n",
        "LEYE = [\n",
        "    263, 249, 390, 373, 374, 380, 381, 382, 362,\n",
        "    466, 388, 387, 386, 385, 384, 398,\n",
        "]\n",
        "LHAND = np.arange(468, 489).tolist()\n",
        "RHAND = np.arange(522, 543).tolist()\n",
        "POINT_LANDMARKS = LIP + LHAND + RHAND + NOSE + REYE + LEYE\n",
        "NUM_NODES = len(POINT_LANDMARKS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def flip_lr(x):\n",
        "    # Assuming x coordinates are normalized [0,1] and x has shape [num_frames, num_landmarks, 3]\n",
        "    x[..., 0] = 1 - x[..., 0]  # Flip the x-coordinate\n",
        "\n",
        "    indices_to_swap = {\n",
        "        tuple(LHAND): RHAND,\n",
        "        tuple(RHAND): LHAND,\n",
        "        tuple(LLIP): RLIP,\n",
        "        tuple(RLIP): LLIP,\n",
        "        tuple(LPOSE): RPOSE,\n",
        "        tuple(RPOSE): LPOSE,\n",
        "        tuple(LEYE): REYE,\n",
        "        tuple(REYE): LEYE,\n",
        "        tuple(LNOSE): RNOSE,\n",
        "        tuple(RNOSE): LNOSE,\n",
        "    }\n",
        "    \n",
        "    num_landmarks = x.shape[1]\n",
        "    \n",
        "    for k, v in indices_to_swap.items():\n",
        "        # Ensure all indices are within bounds before swapping\n",
        "        if max(max(k), max(v)) < num_landmarks:\n",
        "            temp = x[:, k].clone()\n",
        "            x[:, k] = x[:, v]\n",
        "            x[:, v] = temp\n",
        "    \n",
        "    return x\n",
        "\n",
        "def resample(x, rate_range=(0.8, 1.2)):\n",
        "    original_length = x.size(0)\n",
        "    new_length = int(original_length * torch.empty(1).uniform_(*rate_range).item())\n",
        "    indices = torch.linspace(0, original_length - 1, new_length).long()\n",
        "    return x[indices]\n",
        "\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "def spatial_random_affine(x, scale=(0.8, 1.2), shear=(-15, 15), shift=(-0.1, 0.1), degree=(-30, 30)):\n",
        "    # Apply random scaling\n",
        "    scale_factor = torch.empty(1).uniform_(*scale).item()\n",
        "    x[:, :, :2] *= scale_factor  # Scale x, y coordinates\n",
        "\n",
        "    # Apply random rotation\n",
        "    theta = torch.empty(1).uniform_(*degree) * (math.pi / 180)  # Convert degrees to radians\n",
        "    rotation_matrix = torch.tensor([\n",
        "        [torch.cos(theta), -torch.sin(theta)],\n",
        "        [torch.sin(theta), torch.cos(theta)]\n",
        "    ])\n",
        "    x[:, :, :2] = torch.matmul(x[:, :, :2], rotation_matrix)  # Apply rotation\n",
        "\n",
        "    # Apply random translation\n",
        "    translation = torch.empty(2).uniform_(*shift) * x.shape[1]  # Adjust translation range based on shape\n",
        "    x[:, :, :2] += translation\n",
        "\n",
        "    return x\n",
        "\n",
        "def temporal_crop(x, length=MAX_LEN):\n",
        "    l = x.size(0)\n",
        "    if l < length:\n",
        "        # If the sequence is shorter than the desired length, return as is\n",
        "        return x\n",
        "    # Calculate a valid offset to avoid out-of-bounds indexing\n",
        "    offset = torch.randint(0, l - length + 1, (1,)).item()\n",
        "    return x[offset:offset+length]\n",
        "\n",
        "def temporal_mask(x, size_range=(0.2, 0.4), mask_value=float('nan')):\n",
        "    length = x.size(0)\n",
        "    mask_size = int(torch.empty(1).uniform_(*size_range).item() * length)\n",
        "    start = torch.randint(0, length - mask_size + 1, (1,)).item()\n",
        "    x[start:start+mask_size] = mask_value\n",
        "    return x\n",
        "\n",
        "def spatial_mask(x, size_range=(0.2, 0.4), mask_value=float('nan')):\n",
        "    num_landmarks = x.size(1)\n",
        "    num_to_mask = int(num_landmarks * torch.empty(1).uniform_(*size_range).item())\n",
        "    indices_to_mask = torch.randperm(num_landmarks)[:num_to_mask]\n",
        "    x[:, indices_to_mask, :] = mask_value\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "augmentations = [flip_lr, temporal_mask, resample, spatial_random_affine, temporal_crop, spatial_mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_nans_torch(tensor):\n",
        "    # Assumes tensor shape [n_frames, n_landmarks, 3]\n",
        "    # Check for any NaN values across the xyz dimensions of each landmark\n",
        "    is_not_nan = ~torch.isnan(tensor).any(dim=2)  # Check along the xyz dimension\n",
        "    valid_frames = is_not_nan.all(dim=1)  # Check that all landmarks in a frame are not NaN\n",
        "    return tensor[valid_frames]\n",
        "\n",
        "class ParquetDataset(Dataset):\n",
        "    def __init__(self, parquet_paths, id_labels, augmentations=None):\n",
        "        self.parquet_paths = parquet_paths\n",
        "        self.id_labels = id_labels\n",
        "        self.augmentations = augmentations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.parquet_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        parquet_path = self.parquet_paths[idx]\n",
        "        label = self.id_labels[idx]\n",
        "        data = pq.read_table(parquet_path).to_pandas()\n",
        "\n",
        "        # Filter relevant landmarks\n",
        "        filtered_data = data[data['landmark_index'].isin(POINT_LANDMARKS)]\n",
        "\n",
        "        # Extract the columns and group by frame\n",
        "        grouped = filtered_data.groupby('frame')\n",
        "\n",
        "        frames_list = []\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "        z_list = []\n",
        "\n",
        "        for frame, group in grouped:\n",
        "            frames_list.append(frame)\n",
        "            x_list.append(group['x'].values)\n",
        "            y_list.append(group['y'].values)\n",
        "            z_list.append(group['z'].values)\n",
        "        \n",
        "        # Convert to a single combined numpy array\n",
        "        combined_array = np.zeros((len(x_list), NUM_NODES, 3))\n",
        "        for i in range(len(x_list)):\n",
        "            combined_array[i, :len(x_list[i]), 0] = x_list[i]\n",
        "            combined_array[i, :len(y_list[i]), 1] = y_list[i]\n",
        "            combined_array[i, :len(z_list[i]), 2] = z_list[i]\n",
        "\n",
        "        # Convert to tensor before filtering NaNs\n",
        "        combined_tensor = torch.tensor(combined_array, dtype=torch.float32)\n",
        "        filtered_tensor = filter_nans_torch(combined_tensor)\n",
        "\n",
        "        if self.augmentations:\n",
        "            for aug in self.augmentations:\n",
        "                filtered_tensor = aug(filtered_tensor)\n",
        "\n",
        "        filtered_tensor = filter_nans_torch(combined_tensor)\n",
        "\n",
        "        # Ensure the filtered array is padded to MAX_LEN\n",
        "        if filtered_tensor.shape[0] < MAX_LEN:\n",
        "            padded_tensor = torch.full((MAX_LEN, NUM_NODES, 3), PAD, dtype=torch.float32)\n",
        "            padded_tensor[:filtered_tensor.shape[0], :, :] = filtered_tensor\n",
        "        else:\n",
        "            padded_tensor = filtered_tensor[:MAX_LEN, :, :]\n",
        "\n",
        "\n",
        "        return padded_tensor, label\n",
        "\n",
        "def collate_fn(batch):\n",
        "    data, labels = zip(*batch)\n",
        "    data = torch.stack(data)  # Ensure all data tensors have the same size\n",
        "    labels = torch.tensor(labels)\n",
        "    return data, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No NaN values in batch 1\n",
            "Batch 1\n",
            "Data shape: torch.Size([64, 384, 118, 3])\n",
            "No NaN values in batch 2\n",
            "Batch 2\n",
            "Data shape: torch.Size([64, 384, 118, 3])\n",
            "No NaN values in batch 3\n",
            "Batch 3\n",
            "Data shape: torch.Size([64, 384, 118, 3])\n",
            "No NaN values in batch 4\n",
            "Batch 4\n",
            "Data shape: torch.Size([64, 384, 118, 3])\n",
            "No NaN values in batch 5\n",
            "Batch 5\n",
            "Data shape: torch.Size([64, 384, 118, 3])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "dataset = ParquetDataset(paths, ids, augmentations)\n",
        "testloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "for i, batch in enumerate(testloader):\n",
        "    data, labels = batch\n",
        "    if torch.isnan(data).any():\n",
        "        print(f\"NaN values found in batch {i+1}\")\n",
        "    else:\n",
        "        print(f\"No NaN values in batch {i+1}\")\n",
        "    print(f\"Batch {i+1}\")\n",
        "    print(f\"Data shape: {data.shape}\")\n",
        "    if i == 4:  # Print only the first 5 batches\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ECA(nn.Module):\n",
        "    def __init__(self, kernel_size=5):\n",
        "        super(ECA, self).__init__()\n",
        "        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=kernel_size//2, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, T = x.shape  # Batch size, Channels, Time steps\n",
        "        nn = F.adaptive_avg_pool1d(x, 1).view(B, C, 1)\n",
        "        nn = self.conv(nn.transpose(1, 2)).transpose(1, 2)  # Transpose to match Conv1d expected input\n",
        "        nn = torch.sigmoid(nn)\n",
        "        return x * nn\n",
        "\n",
        "class LateDropout(nn.Module):\n",
        "    def __init__(self, rate, start_step=0):\n",
        "        super(LateDropout, self).__init__()\n",
        "        self.rate = rate\n",
        "        self.start_step = start_step\n",
        "        self.step = 0\n",
        "\n",
        "    def forward(self, x, training=False):\n",
        "        if training and self.step >= self.start_step:\n",
        "            x = F.dropout(x, p=self.rate, training=training)\n",
        "        if training:\n",
        "            self.step += 1\n",
        "        return x\n",
        "\n",
        "class CausalDWConv1D(nn.Module):\n",
        "    def __init__(self, in_channels, kernel_size=17, dilation_rate=1, use_bias=False):\n",
        "        super(CausalDWConv1D, self).__init__()\n",
        "        self.causal_pad = nn.ConstantPad1d((dilation_rate * (kernel_size - 1), 0), 0)\n",
        "        self.dw_conv = nn.Conv1d(in_channels, in_channels, kernel_size, dilation=dilation_rate, bias=use_bias, groups=in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.causal_pad(x)\n",
        "        x = self.dw_conv(x)\n",
        "        return x\n",
        "\n",
        "import torch.nn.functional as F  \n",
        "\n",
        "class Conv1DBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, dilation_rate=1, drop_rate=0.0, expand_ratio=2, activation='relu'):\n",
        "        super(Conv1DBlock, self).__init__()\n",
        "        self.expand = nn.Conv1d(in_channels, in_channels * expand_ratio, kernel_size=1, bias=True)\n",
        "        self.dw_conv = CausalDWConv1D(in_channels * expand_ratio, kernel_size, dilation_rate)\n",
        "        self.bn = nn.BatchNorm1d(in_channels * expand_ratio)\n",
        "        self.eca = ECA(kernel_size)\n",
        "        self.project = nn.Conv1d(in_channels * expand_ratio, out_channels, kernel_size=1, bias=True)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        self.activation = getattr(F, activation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.activation(self.expand(x))\n",
        "        x = self.dw_conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.eca(x)\n",
        "        x = self.project(x)\n",
        "        if self.drop.p > 0:\n",
        "            x = self.drop(x)\n",
        "        if residual.shape == x.shape:\n",
        "            x += residual\n",
        "        return x\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, dim=256, num_heads=4, dropout=0):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.dim = dim\n",
        "        self.scale = dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, 3 * dim, bias=False)\n",
        "        self.proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='relu'):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attn = MultiHeadSelfAttention(dim, num_heads, attn_dropout)\n",
        "        self.drop1 = nn.Dropout(drop_rate)\n",
        "        self.norm1 = nn.LayerNorm(dim)  # Using LayerNorm instead of BatchNorm\n",
        "        self.expand = nn.Linear(dim, dim * expand, bias=False)\n",
        "        self.activation = getattr(F, activation)\n",
        "        self.project = nn.Linear(dim * expand, dim, bias=False)\n",
        "        self.drop2 = nn.Dropout(drop_rate)\n",
        "        self.norm2 = nn.LayerNorm(dim)  # Using LayerNorm instead of BatchNorm\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out = self.attn(self.norm1(x))\n",
        "        x = x + self.drop1(attn_out)\n",
        "        residual = x\n",
        "        x = self.activation(self.expand(self.norm2(x)))\n",
        "        x = self.project(x)\n",
        "        x = residual + self.drop2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_frames=384, num_landmarks=118, num_classes=250, dim=192, dropout_step=0):\n",
        "        super(Model, self).__init__()\n",
        "        self.masking_value = -100.0\n",
        "        self.embedding = nn.Linear(num_landmarks * 3, dim, bias=False)\n",
        "        self.bn = nn.BatchNorm1d(dim)\n",
        "        self.conv1 = Conv1DBlock(dim, dim, 17, drop_rate=0.2)\n",
        "        self.conv2 = Conv1DBlock(dim, dim, 17, drop_rate=0.2)\n",
        "        self.conv3 = Conv1DBlock(dim, dim, 17, drop_rate=0.2)\n",
        "        self.trans1 = TransformerBlock(dim, expand=2)\n",
        "        self.conv4 = Conv1DBlock(dim, dim, 17, drop_rate=0.2)\n",
        "        self.conv5 = Conv1DBlock(dim, dim, 17, drop_rate=0.2)\n",
        "        self.conv6 = Conv1DBlock(dim, dim, 17, drop_rate=0.2)\n",
        "        self.trans2 = TransformerBlock(dim, expand=2)\n",
        "        self.fc = nn.Linear(dim, num_classes)\n",
        "        self.late_dropout = LateDropout(0.8, start_step=dropout_step)\n",
        "        \n",
        "    def forward(self, x, training=False):\n",
        "        B, T, L, C = x.shape  # [batch_size, num_frames, num_landmarks, 3]\n",
        "        x = x.reshape(B, T, L * C)  # Flatten landmarks and coordinates into the feature dimension\n",
        "        mask = (x != self.masking_value).float()\n",
        "        x = self.embedding(x)\n",
        "        x = self.bn(x.transpose(1, 2)).transpose(1, 2)  # Apply batch norm on the feature dimension\n",
        "        x = x.transpose(1, 2)  # [batch_size, dim, num_frames] for Conv1D\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.trans1(x.transpose(1, 2)).transpose(1, 2)  # [batch_size, num_frames, dim]\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.trans2(x.transpose(1, 2)).transpose(1, 2)  # [batch_size, num_frames, dim]\n",
        "        x = F.adaptive_avg_pool1d(x, 1).squeeze(-1)  # [batch_size, dim]\n",
        "        x = self.late_dropout(x, training=training)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1:\n",
            "Train loader has 68024 samples\n",
            "Val loader has 17006 samples\n",
            "Fold 2:\n",
            "Train loader has 68024 samples\n",
            "Val loader has 17006 samples\n",
            "Fold 3:\n",
            "Train loader has 68024 samples\n",
            "Val loader has 17006 samples\n",
            "Fold 4:\n",
            "Train loader has 68024 samples\n",
            "Val loader has 17006 samples\n",
            "Fold 5:\n",
            "Train loader has 68024 samples\n",
            "Val loader has 17006 samples\n",
            "Test loader has 9447 samples\n",
            "Training fold 1\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[70], line 154\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    153\u001b[0m cfg \u001b[38;5;241m=\u001b[39m CFG()\n\u001b[0;32m--> 154\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[70], line 148\u001b[0m, in \u001b[0;36mtrain_folds\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# Assuming `train_fold` is adapted to take loaders directly\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted training for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "Cell \u001b[0;32mIn[70], line 74\u001b[0m, in \u001b[0;36mtrain_fold\u001b[0;34m(cfg, fold, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_fold\u001b[39m(cfg, fold, train_loader, val_loader, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 74\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[1;32m     76\u001b[0m         scaler \u001b[38;5;241m=\u001b[39m amp\u001b[38;5;241m.\u001b[39mGradScaler()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/cuda/__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "class CFG:\n",
        "    n_splits = 5\n",
        "    save_output = True\n",
        "    output_dir = '.'\n",
        "\n",
        "    seed = 42\n",
        "    verbose = 2 #0) silent 1) progress bar 2) one line per epoch\n",
        "\n",
        "    max_len = 384\n",
        "    replicas = 8\n",
        "    lr = 5e-4 * replicas\n",
        "    weight_decay = 0.1\n",
        "    lr_min = 1e-6\n",
        "    epochs = 300 #400\n",
        "    warmup = 0\n",
        "    batch_size = 64 * replicas\n",
        "    snapshot_epochs = []\n",
        "    swa_epochs = [] #list(range(epoch//2,epoch+1))\n",
        "\n",
        "    fp16 = True\n",
        "    fgm = False\n",
        "    awp = True\n",
        "    awp_lambda = 0.2\n",
        "    awp_start_epoch = 15\n",
        "    dropout_start_epoch = 15\n",
        "    resume = 0\n",
        "    decay_type = 'cosine'\n",
        "    dim = 192\n",
        "    comment = f'islr-fp16-192-8-seed{seed}'\n",
        "\n",
        "def get_dataloaders(dataset, num_folds, batch_size, test_size_ratio=0.1):\n",
        "    indices = np.arange(len(dataset))\n",
        "    test_size = int(test_size_ratio * len(dataset))\n",
        "    test_indices = np.random.choice(indices, test_size, replace=False)\n",
        "\n",
        "    train_val_indices = np.setdiff1d(indices, test_indices)\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    fold_data = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(train_val_indices):\n",
        "        train_idx = train_val_indices[train_idx]\n",
        "        val_idx = train_val_indices[val_idx]\n",
        "        \n",
        "        train_subset = Subset(dataset, train_idx)\n",
        "        val_subset = Subset(dataset, val_idx)\n",
        "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "        \n",
        "        fold_data.append((train_loader, val_loader))\n",
        "    \n",
        "    test_subset = Subset(dataset, test_indices)\n",
        "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    return fold_data, test_loader\n",
        "\n",
        "# Parameters\n",
        "num_folds = 5\n",
        "batch_size = 64 * 8\n",
        "\n",
        "# Split the dataset indices for k-fold cross-validation\n",
        "indices = np.arange(len(dataset))\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "fold_data, test_loader = get_dataloaders(dataset, num_folds, batch_size)\n",
        "\n",
        "for fold, (train_loader, val_loader) in enumerate(fold_data):\n",
        "    print(f\"Fold {fold + 1}:\")\n",
        "    print(f\"Train loader has {len(train_loader.dataset)} samples\")\n",
        "    print(f\"Val loader has {len(val_loader.dataset)} samples\")\n",
        "print(f\"Test loader has {len(test_loader.dataset)} samples\")\n",
        "\n",
        "def train_fold(cfg, fold, train_loader, val_loader, device='cuda'):\n",
        "    model = Model(cfg.dim).to(device)\n",
        "    if cfg.fp16:\n",
        "        scaler = amp.GradScaler()\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    scheduler = OneCycleLR(optimizer, max_lr=cfg.lr, epochs=cfg.epochs, steps_per_epoch=len(train_loader))\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        iteration = 0\n",
        "        print(f\"Starting training for epoch {epoch+1}/{cfg.epochs}, fold {fold+1}\")\n",
        "        \n",
        "        for data, target in train_loader:\n",
        "            iteration += 1\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            with amp.autocast(enabled=cfg.fp16):\n",
        "                output = model(data)\n",
        "                loss = torch.nn.functional.cross_entropy(output, target)\n",
        "            \n",
        "            if cfg.fp16:\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if iteration % 100 == 0:\n",
        "                print(f\"    Epoch {epoch+1}, Fold {fold+1}, Iteration {iteration}, Partial Loss: {running_loss / iteration:.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f\"Training completed for epoch {epoch+1}/{cfg.epochs}, fold {fold+1}. Average Loss: {running_loss / len(train_loader):.4f}\")\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        validation_loss = 0.0\n",
        "        val_iteration = 0\n",
        "        print(f\"Starting validation for epoch {epoch+1}/{cfg.epochs}, fold {fold+1}\")\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                val_iteration += 1\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                loss = torch.nn.functional.cross_entropy(output, target)\n",
        "                validation_loss += loss.item()\n",
        "                \n",
        "                if val_iteration % 100 == 0:\n",
        "                    print(f\"    Epoch {epoch+1}, Fold {fold+1}, Validation Iteration {val_iteration}, Partial Val Loss: {validation_loss / val_iteration:.4f}\")\n",
        "\n",
        "        avg_val_loss = validation_loss / len(val_loader)\n",
        "        print(f\"Validation completed for epoch {epoch+1}/{cfg.epochs}, fold {fold+1}. Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Save the best model\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), f'{cfg.output_dir}/{cfg.comment}-fold{fold}-best.pth')\n",
        "            print(f\"New best model saved with validation loss {avg_val_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_folds(cfg):\n",
        "    # Assuming `get_dataloaders` returns all the loaders including the test loader\n",
        "    fold_data, test_loader = get_dataloaders(dataset, cfg.n_splits, cfg.batch_size)\n",
        "\n",
        "    for fold, (train_loader, val_loader) in enumerate(fold_data):\n",
        "        print(f\"Training fold {fold + 1}\")\n",
        "        # Assuming `train_fold` is adapted to take loaders directly\n",
        "        model = train_fold(cfg, fold, train_loader, val_loader)\n",
        "        print(f\"Completed training for fold {fold + 1}\")\n",
        "    \n",
        "    return model\n",
        "        \n",
        "cfg = CFG()\n",
        "model = train_folds(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader, criterion, cfg, device='cuda'):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            print('-' * 50)\n",
        "            print('test iteration', i)\n",
        "            i += 1\n",
        "            print('inputs ', inputs.shape)\n",
        "            print('outputs ', outputs.shape)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_predictions += torch.sum(preds == labels.data).float()  # Ensure float32 for correct accumulation\n",
        "\n",
        "    test_loss = running_loss / len(test_loader.dataset)\n",
        "    accuracy = correct_predictions / len(test_loader.dataset)\n",
        "    return test_loss, accuracy\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "print('=' * 50)\n",
        "test_loss, test_accuracy = evaluate(model, test_loader, criterion, cfg)\n",
        "print('=' * 50)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print('=' * 50)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
